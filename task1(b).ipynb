{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPf/XX9vtZV2tUJ8m7+moMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeshsingh5505/pdfExtract/blob/main/task1(b).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3JXDMl-qHh0I",
        "outputId": "6bc98a48-8a8b-430f-c00d-5560ee14c2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def extract_blocks_features_to_csv(pdf_path, output_csv):\n",
        "    blocks = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages, 1):\n",
        "            last_bottom = 0\n",
        "            words = page.extract_words(extra_attrs=[\"fontname\", \"size\", \"adv\", \"upright\"])\n",
        "\n",
        "            # Group by Y coordinate (approximate lines)\n",
        "            lines = {}\n",
        "            for w in words:\n",
        "                top_rounded = round(w['top'] / 3) * 3  # lines within ~3pt\n",
        "                lines.setdefault(top_rounded, []).append(w)\n",
        "\n",
        "            for top in sorted(lines):\n",
        "                line_words = lines[top]\n",
        "                text = \" \".join(w[\"text\"] for w in line_words)\n",
        "                font_sizes = [w[\"size\"] for w in line_words if w.get(\"size\")]\n",
        "                fontnames = [w[\"fontname\"] for w in line_words if w.get(\"fontname\")]\n",
        "                is_bold_list = [1 if (\"Bold\" in str(f) or \"bold\" in str(f).lower()) else 0 for f in fontnames]\n",
        "\n",
        "                font_size = max(font_sizes) if font_sizes else 0\n",
        "                font_name = fontnames[0] if fontnames else \"\"\n",
        "                is_bold = max(is_bold_list) if is_bold_list else 0\n",
        "\n",
        "                stripped_text = text.strip()\n",
        "                if stripped_text.isupper():\n",
        "                    text_case = \"ALLCAPS\"\n",
        "                elif stripped_text.istitle():\n",
        "                    text_case = \"Title\"\n",
        "                elif stripped_text.islower():\n",
        "                    text_case = \"lower\"\n",
        "                else:\n",
        "                    text_case = \"mixed\"\n",
        "\n",
        "                # Heuristic: numbered prefix pattern\n",
        "                prefix_pattern = 0\n",
        "                if re.match(r\"^(\\d+\\.)+(\\s|$)\", stripped_text):\n",
        "                    prefix_pattern = 1\n",
        "\n",
        "                whitespace_above = max(0, top - last_bottom) if last_bottom else top\n",
        "                line_length = len(stripped_text.split())\n",
        "\n",
        "                blocks.append({\n",
        "                    \"page_num\": page_num,\n",
        "                    \"text\": text,\n",
        "                    \"font_size\": font_size,\n",
        "                    \"font_name\": font_name,\n",
        "                    \"is_bold\": is_bold,\n",
        "                    \"y0\": top,\n",
        "                    \"whitespace_above\": whitespace_above,\n",
        "                    \"prefix_pattern\": prefix_pattern,\n",
        "                    \"text_case\": text_case,\n",
        "                    \"line_length\": line_length,\n",
        "                    \"label\": \"\"  # fill this manually (\"Title\", \"H1\", \"H2\", etc.) after export\n",
        "                })\n",
        "\n",
        "                last_bottom = top\n",
        "\n",
        "    df = pd.DataFrame(blocks)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Extracted {len(df)} rows to {output_csv}\")\n",
        "\n",
        "# Usage\n",
        "extract_blocks_features_to_csv(\"/content/Gmail - Application Submit.pdf\", \"train_blocks_unlabeled.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iTkjLgqSIv4f",
        "outputId": "baf1df27-40c5-466b-e230-6b3111352e68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 37 rows to train_blocks_unlabeled.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_n1RDjYAPVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction**"
      ],
      "metadata": {
        "id": "5UQFQamYKrcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pickle\n",
        "\n",
        "# Corrected extract_blocks_features function\n",
        "def extract_blocks_features(pdf_path):\n",
        "    blocks = []\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            last_y0 = 0  # Track last y0 for whitespace calculation\n",
        "            for page_num, page in enumerate(pdf.pages):\n",
        "                for obj in page.extract_words(extra_attrs=[\"fontname\", \"size\"]):\n",
        "                    text = obj[\"text\"].strip()\n",
        "                    if not text:  # Skip empty text\n",
        "                        continue\n",
        "                    block = {\n",
        "                        \"text\": text,\n",
        "                        \"font_size\": obj[\"size\"],\n",
        "                        \"font_name\": obj[\"fontname\"],\n",
        "                        \"is_bold\": 1 if \"Bold\" in obj[\"fontname\"] or \"bold\" in obj[\"fontname\"].lower() else 0,\n",
        "                        \"y0\": obj[\"top\"],\n",
        "                        \"page_num\": page_num + 1,\n",
        "                        \"line_length\": len(text.split()),\n",
        "                        \"whitespace_above\": obj[\"top\"] - last_y0 if blocks else obj[\"top\"],\n",
        "                    }\n",
        "                    # Prefix pattern\n",
        "                    block[\"prefix_pattern\"] = 1 if text.split() and text.split()[0].rstrip(\".\").replace(\".\", \"\").isdigit() else 0\n",
        "                    # Text case\n",
        "                    block[\"text_case\"] = (\"ALLCAPS\" if text.isupper() else\n",
        "                                         \"Title\" if text.istitle() else\n",
        "                                         \"lower\" if text.islower() else \"mixed\")\n",
        "                    blocks.append(block)\n",
        "                    last_y0 = obj[\"top\"]\n",
        "        return pd.DataFrame(blocks)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: PDF file {pdf_path} not found.\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load and train model (assuming CSV is correctly formatted)\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/train_blocks_unlabeled (4).csv\")\n",
        "    df = pd.get_dummies(df, columns=[\"font_name\", \"text_case\"])\n",
        "    X = df.drop([\"label\", \"text\"], axis=1)\n",
        "    y = df[\"label\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    dtree = DecisionTreeClassifier(max_depth=6, min_samples_leaf=8, random_state=42)\n",
        "    dtree.fit(X_train, y_train)\n",
        "    print(f\"Validation Accuracy: {dtree.score(X_test, y_test):.2f}\")\n",
        "\n",
        "    # Save training columns for inference\n",
        "    with open(\"training_columns.pkl\", \"wb\") as f:\n",
        "        pickle.dump(X.columns.tolist(), f)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Training CSV file not found.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Inference on new PDF\n",
        "try:\n",
        "    new_df = extract_blocks_features(\"/content/file05.pdf\")\n",
        "    if new_df.empty:\n",
        "        print(\"Error: No data extracted from PDF.\")\n",
        "        exit()\n",
        "\n",
        "    new_df = pd.get_dummies(new_df, columns=[\"font_name\", \"text_case\"])\n",
        "\n",
        "    # Load training columns\n",
        "    with open(\"training_columns.pkl\", \"rb\") as f:\n",
        "        training_columns = pickle.load(f)\n",
        "\n",
        "    # Align columns\n",
        "    new_X = new_df.reindex(columns=training_columns, fill_value=0)\n",
        "    predicted_labels = dtree.predict(new_X)\n",
        "\n",
        "    results = new_df[[\"page_num\", \"text\"]].copy()\n",
        "    results[\"predicted_label\"] = predicted_labels\n",
        "    print(results)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW0mZD9yIAzh",
        "outputId": "40e614c8-7142-4736-d450-14378cfe6772"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.96\n",
            "    page_num              text predicted_label\n",
            "0          1          ADDRESS:           Other\n",
            "1          1           TOPJUMP           Other\n",
            "2          1              3735           Other\n",
            "3          1           PARKWAY           Other\n",
            "4          1            PIGEON           Other\n",
            "5          1            FORGE,           Other\n",
            "6          1                TN           Other\n",
            "7          1             37863           Other\n",
            "8          1             (NEAR           Other\n",
            "9          1             DIXIE           Other\n",
            "10         1          STAMPEDE           Other\n",
            "11         1                ON           Other\n",
            "12         1               THE           Other\n",
            "13         1          PARKWAY)           Other\n",
            "14         1             RSVP:           Other\n",
            "15         1  ----------------              H2\n",
            "16         1            CLOSED           Other\n",
            "17         1              TOED           Other\n",
            "18         1             SHOES           Other\n",
            "19         1               ARE           Other\n",
            "20         1          REQUIRED           Other\n",
            "21         1               FOR           Other\n",
            "22         1          CLIMBING           Other\n",
            "23         1           PARENTS           Other\n",
            "24         1                OR           Other\n",
            "25         1         GUARDIANS           Other\n",
            "26         1               NOT           Other\n",
            "27         1         ATTENDING           Other\n",
            "28         1               THE           Other\n",
            "29         1            PARTY,           Other\n",
            "30         1            PLEASE           Other\n",
            "31         1             VISIT           Other\n",
            "32         1       TOPJUMP.COM           Other\n",
            "33         1                TO           Other\n",
            "34         1              FILL           Other\n",
            "35         1               OUT           Other\n",
            "36         1            WAIVER           Other\n",
            "37         1                SO           Other\n",
            "38         1              YOUR           Other\n",
            "39         1             CHILD           Other\n",
            "40         1               CAN           Other\n",
            "41         1           ATTEND.           Other\n",
            "42         1              HOPE              H2\n",
            "43         1                To              H2\n",
            "44         1               SEE              H2\n",
            "45         1               You              H2\n",
            "46         1            THERE!              H2\n",
            "47         1   WWW.TOPJUMP.COM              H3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "btXJtxRDKokB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/train_blocks_unlabeled (4).csv\")  # Hand-labeled dataset\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, columns=[\"font_name\", \"text_case\"])\n",
        "X = df.drop([\"label\", \"text\"], axis=1)\n",
        "y = df[\"label\"]\n"
      ],
      "metadata": {
        "id": "n1ZuwJPHKn0_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, random_state=42)\n",
        "\n",
        "dtree = DecisionTreeClassifier(max_depth=6, min_samples_leaf=8, random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Validation Accuracy: {dtree.score(X_test, y_test):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PpTvb2yKw_p",
        "outputId": "48956a27-bec6-46b5-9bb4-a58418a6eea5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extraction on new files**"
      ],
      "metadata": {
        "id": "KohmtOu0K1-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For inference on new PDF\n",
        "new_df = extract_blocks_features(\"/content/file02.pdf\")\n",
        "new_df = pd.get_dummies(new_df, columns=[\"font_name\", \"text_case\"])\n",
        "\n",
        "# Ensure columns align with training\n",
        "new_X = new_df.reindex(columns=X.columns, fill_value=0)\n",
        "predicted_labels = dtree.predict(new_X)\n",
        "\n",
        "results = new_df[[\"page_num\", \"text\"]].copy()\n",
        "results[\"predicted_label\"] = predicted_labels\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlKtYa_XK1Xt",
        "outputId": "e704b475-a764-4629-ca7e-1eca7209a5cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      page_num            text predicted_label\n",
            "0            1        Overview              H3\n",
            "1            1      Foundation              H3\n",
            "2            1           Level              H3\n",
            "3            1      Extensions              H3\n",
            "4            1         Version           Other\n",
            "...        ...             ...             ...\n",
            "2381        12   International           Other\n",
            "2382        12        Software           Other\n",
            "2383        12         Testing           Other\n",
            "2384        12  Qualifications           Other\n",
            "2385        12           Board           Other\n",
            "\n",
            "[2386 rows x 3 columns]\n"
          ]
        }
      ]
    }
  ]
}
